---
name: phase4-results-diagnoser
description: Inspect and diagnose training_summary.json from NeuroMF diffusion training runs. Use when the user asks to check training progress, diagnose issues, or analyse logs. This skill provides the mathematical context for each logged metric so you can reason scientifically about training dynamics rather than follow a checklist.
argument-hint: "<run-name, e.g. 'run_v1', 'run_v2'>"
---

# Training Diagnostics — NeuroMF Phase 4

## Purpose

You are analysing training telemetry from a latent-space diffusion model (NeuroMF) that uses the **improved MeanFlow (iMF)** formulation with **x-prediction** and **per-channel Lp loss** to generate 3D brain MRI volumes in a single network function evaluation (1-NFE). The model operates in the 4x48^3 latent space of a frozen MAISI-V2 VAE-GAN.

**Your job is to reason like a scientist**: understand what each metric measures physically, identify trends and correlations across metrics, form hypotheses about what the model is learning (or failing to learn), and present conclusions with supporting evidence. Do not follow a fixed rubric — think.

---

## Data Location

The run to analyse is `$ARGUMENTS`. The data lives at:

```
/media/mpascual/Sandisk2TB/research/neuromf/results/phase_4/runs/$ARGUMENTS/
```

Key files:
- `diagnostics/aggregate_results/training_summary.json` — **Primary data source.** JSON array of per-epoch summaries.
- `diagnostics/per_epoch_metrics/epoch_NNN/summary.json` — Per-epoch snapshots (same data, one file per epoch).
- `samples/generated_samples/epoch_NNNN.pt` — **Latent archives.** Raw generated latents at multiple NFE steps with fixed noise for evolution tracking. See Generated Samples section below.
- `samples/epoch_NNNN/latent_channels.png` — Latent channel visualization (4×3 grid, no VAE).
- `samples/decoded/` — Post-training VAE-decoded images (generated by `decode_samples.py`).
- `logs/` — TensorBoard event files (only if you need step-level resolution beyond epoch summaries).
- `checkpoints/` — Model checkpoints (for weight analysis if needed).

**Note:** Older runs (before v2) may have the flat layout `diagnostics/training_summary.json` and `diagnostics/epoch_NNN/` instead. Check which layout exists before accessing files. Runs before v3 may have `samples/epoch_NNNN/samples_grid.png` from inline VAE decoding instead of the latent archive format.

The codebase is at `/home/mpascual/research/code/neuromf/`. Use `~/.conda/envs/neuromf/bin/python` to run any Python scripts.

---

## How to Explore the JSON Efficiently

**CRITICAL**: `training_summary.json` can be 10,000+ lines. **Do NOT** read the whole file with the Read tool. Instead, write small Python scripts to extract what you need. Here are patterns to use:

```python
# Load and get overview
import json
with open("...training_summary.json") as f:
    data = json.load(f)
print(f"Epochs: {len(data)}, Keys: {sorted(data[0].keys())}")
```

```python
# Extract a single metric's trajectory
vals = [e["raw_loss_mean"] for e in data if "raw_loss_mean" in e]
print(f"raw_loss_mean: start={vals[0]:.4f}, end={vals[-1]:.4f}, "
      f"ratio={vals[-1]/vals[0]:.4f}")
```

```python
# Compare two metrics over time
import numpy as np
epochs = [e["epoch"] for e in data]
m1 = [e.get("cosine_sim_V_vc", float("nan")) for e in data]
m2 = [e.get("relative_error", float("nan")) for e in data]
corr = np.corrcoef(m1, m2)[0, 1]
print(f"Pearson r(cosine_sim_V_vc, relative_error) = {corr:.4f}")
```

```python
# Windowed statistics (detect phase transitions)
import numpy as np
vals = np.array([e["raw_loss_mean"] for e in data if "raw_loss_mean" in e])
win = 20
means = np.convolve(vals, np.ones(win)/win, mode='valid')
print(f"Rolling mean (window={win}): start={means[0]:.4f}, "
      f"mid={means[len(means)//2]:.4f}, end={means[-1]:.4f}")
```

```python
# Extract nested dict values (velocity norms, x_hat_stats)
u_norms = [e["velocity_norms"]["u_norm"] for e in data if "velocity_norms" in e]
target_norms = [e["velocity_norms"]["target_v_norm"] for e in data if "velocity_norms" in e]
ratios = [u/t for u, t in zip(u_norms, target_norms)]
print(f"u/target ratio: start={ratios[0]:.4f}, end={ratios[-1]:.4f}")
```

```python
# Multi-panel diagnostic plot
import matplotlib
matplotlib.use("Agg")
import matplotlib.pyplot as plt
import json, numpy as np

with open("...training_summary.json") as f:
    data = json.load(f)
epochs = [e["epoch"] for e in data]

fig, axes = plt.subplots(3, 2, figsize=(14, 12), sharex=True)
# ... fill in panels ...
fig.savefig("/tmp/diagnostics.png", dpi=150, bbox_inches="tight")
print("Saved to /tmp/diagnostics.png")
```

**Workflow**: Start by loading the JSON once and printing the keys of the first and last epoch. Then extract individual metrics of interest with targeted scripts. Build up to multi-metric comparisons and plots only after you understand the individual trajectories.

---

## JSON Schema and Metric Semantics

Each element of the JSON array is one epoch. Below is the full schema with the physical meaning of every field.

### Top-level Identifiers

| Field | Type | Meaning |
|-------|------|---------|
| `epoch` | int | 0-indexed epoch number |
| `global_step` | int | Total optimizer steps so far (includes accumulation) |

### Loss Metrics

#### `loss_mean` / `loss_std` — Adaptive-Weighted Loss

The epoch mean/std of: `L = w(t) * ||V - v_c||_p^p`, where `w(t) = 1 / (sg[||error||^p] + eps)`.

**The adaptive weight makes this value almost constant** (~1.0 with norm_eps=1.0). By design (pMF Section A.1), this stabilises training across timesteps but means the absolute loss value carries almost no information about model quality. Do NOT use `loss_mean` to judge whether the model is learning. Use `raw_loss_mean` instead.

#### `raw_loss_mean` / `raw_loss_std` — Pre-Adaptive Raw Loss

**This is the single most important metric.** The epoch mean/std of `||V - v_c||_p^p` *before* adaptive weighting. This directly measures how well the compound velocity V matches the target conditional velocity v_c = eps - z_0.

- **Decreasing** = the model is learning to predict the velocity field.
- **Increasing** = divergence (bad hyperparameters, learning rate too high, etc.)
- **Flat** = the model is stuck (learning rate too low, wrong architecture, etc.)

Absolute values depend on the spatial dimensions (4x48^3 = 442,368 elements), so interpret relative changes (start-to-end ratio) rather than absolute magnitude.

#### `raw_loss_fm_mean` / `raw_loss_mf_mean` — FM vs MF Loss Split

The raw loss averaged separately over:
- **FM samples** (r = t, flow matching only): The JVP correction vanishes; V = u. This is the standard flow matching loss.
- **MF samples** (r < t, MeanFlow active): V = u + (t-r) * sg[du/dt]. The self-consistency constraint is active.

**Key diagnostic**: If `raw_loss_fm_mean` decreases but `raw_loss_mf_mean` stays flat or increases, the model is learning the velocity field but the JVP correction (du/dt) is not being learned correctly. This can indicate problems with the JVP strategy (exact vs finite-difference) or with the tangent quality.

The FM loss should always be lower than the MF loss, because FM is a simpler objective. A ratio `raw_loss_mf / raw_loss_fm > 10` at convergence suggests the MF term needs more training or a different data_proportion.

### Velocity Norms

Stored in `velocity_norms` dict with keys:

| Key | Symbol | What it measures |
|-----|--------|-----------------|
| `u_norm` | `||u||` | L2 norm of the average velocity u = (z_t - x_hat) / t |
| `compound_v_norm` | `||V||` | L2 norm of compound velocity V = u + (t-r)*sg[du/dt] |
| `target_v_norm` | `||v_c||` | L2 norm of target v_c = eps - z_0 |
| `v_tilde_norm` | `||v_tilde||` | L2 norm of JVP tangent (model's own v_tilde = u(z_t,t,t)) |
| `jvp_norm` | `||du/dt||` | L2 norm of the JVP output (MF samples only) |

**Key relationships to check:**

1. **`u_norm / target_v_norm` ratio**: Should converge toward 1.0. If the model predicts the right magnitude of the velocity, these norms match. If `u_norm >> target_v_norm`, the model is overshooting. If `u_norm << target_v_norm`, it's underestimating.

2. **`compound_v_norm` vs `target_v_norm`**: V is the actual quantity being regressed to v_c. Their norms should converge. More informative than the individual norms.

3. **`v_tilde_norm` vs `u_norm`**: v_tilde is the model's instantaneous velocity u(z_t, t, t), while u is the average velocity u(z_t, t, r). For FM samples (r=t), these are identical. For MF samples, they differ. If they diverge dramatically, the model is inconsistent.

4. **`jvp_norm` trajectory**: Early training: large and noisy (model doesn't know the derivative). Mid training: should stabilise. Late training: should be proportional to the velocity magnitude. If `jvp_norm` keeps growing while `u_norm` stabilises, the model's derivative estimate is unstable.

### Direction Alignment

| Field | Formula | Range | What it measures |
|-------|---------|-------|-----------------|
| `cosine_sim_V_vc` | `cos(V, v_c)` | [-1, 1] | Are V and target pointing in the same direction? |
| `cosine_sim_vtilde_vc` | `cos(v_tilde, v_c)` | [-1, 1] | Is the JVP tangent aligned with the target? |

**These are the most interpretable metrics.** Magnitude (norms) can be compensated by the loss, but direction must be correct for generation to work.

- `cosine_sim_V_vc` should converge toward 1.0. Values below 0.5 at convergence indicate the model is still predicting in wrong directions. Values near 0 mean the model's output is essentially orthogonal to the target — no useful signal.
- `cosine_sim_vtilde_vc` measures the quality of the JVP tangent. Since v_tilde serves as the tangent direction for computing du/dt, its alignment with the ground-truth velocity determines the quality of the MeanFlow correction. This should improve over training as the model's own predictions become more accurate.

**Important**: At random initialisation, cosine similarity in 442,368 dimensions should be ~0 (orthogonal). Any value significantly above 0 at epoch 0 would be suspicious.

### Relative Prediction Error

| Field | Formula | What it measures |
|-------|---------|-----------------|
| `relative_error` | `||V - v_c|| / ||v_c||` | Normalised error (scale-invariant) |

This is the ratio of absolute error to signal magnitude. Interpretable as "how many percent off is the prediction":
- 1.0 = error equals the signal (random-level)
- 0.5 = error is half the signal (mediocre)
- 0.1 = error is 10% of signal (good)
- 0.01 = error is 1% of signal (excellent)

### Adaptive Weight Statistics

Stored in `adaptive_weight` dict:

| Key | Formula | What it measures |
|-----|---------|-----------------|
| `mean` | `mean(sg[||error||^p] + eps)` | Average normalisation denominator |
| `std` | `std(...)` | Spread of normalisation across batch |

**The adaptive weight mean tracks the raw error magnitude.** If `adaptive_weight.mean` grows over training, the raw errors are growing (divergence masked by adaptive normalisation!). If it shrinks, the model is improving. This was the key signal that revealed the Phase 4c failure: `loss_mean` stayed at ~1.0 while `adaptive_weight.mean` grew by 4,808x.

### x-hat Statistics (x-prediction mode)

Stored in `x_hat_stats` dict:

| Key | What it measures |
|-----|-----------------|
| `mean` | Mean of predicted x_0 across batch. Should be near 0 if latents are normalised. |
| `std` | Std of predicted x_0. Should match the std of the training latents (~1.0 if normalised). |
| `min` / `max` | Extremes of predictions. Exploding values = mode collapse or divergence. |

**Physical meaning**: x_hat is the model's prediction of the clean latent z_0. If the model is working, x_hat statistics should resemble the statistics of the training data. If `x_hat.std` keeps growing or `x_hat.min/max` become extreme, the model is generating nonsense.

### Gradient Metrics

| Field | Formula | What it measures |
|-------|---------|-----------------|
| `grad_clip_fraction` | `count(||grad|| > clip) / N` | Fraction of steps where gradients were clipped |
| `grad_norm_mean` | `mean(||grad||)` | Average gradient L2 norm over the epoch |
| `grad_norm_std` | `std(||grad||)` | Gradient norm variability |
| `block_grad_norms` | `{block: RMS(||grad_block||)}` | Per-block RMS gradient norms |

The gradient clip threshold is `gradient_clip_norm: 1.0`.

**`block_grad_norms` block guide:**

| Block | Architecture Role | Expected gradient magnitude |
|-------|-------------------|-----------------------------|
| `conv_in` | 4-ch latent -> 64-ch features | Large (close to loss) |
| `time_embed` | t -> MLP -> conditioning | Medium (via chain rule through all blocks) |
| `r_embed` | r -> MLP -> conditioning | Medium; should grow as MF learns |
| `down_block_0` | 64-ch, 48^3 (highest res) | Large (skip connections) |
| `down_block_1` | 128-ch, 24^3 | Medium |
| `down_block_2` | 256-ch, 12^3, self-attention | Medium-small |
| `down_block_3` | 512-ch, 6^3, self-attention | Small (deepest) |
| `middle_block` | 512-ch, 6^3, bottleneck | Smallest (deepest point) |
| `up_block_0` | 512-ch, 6^3 (mirrors down_3) | Small |
| `up_block_1` | 256-ch, 12^3 | Medium-small |
| `up_block_2` | 128-ch, 24^3 | Medium |
| `up_block_3` | 64-ch, 48^3 (mirrors down_0) | Large (skip connections) |
| `out` | 64-ch -> 4-ch x-prediction | Largest (directly connected to loss) |

### Model Dynamics

| Field | Formula | What it measures |
|-------|---------|-----------------|
| `relative_update_norm` | `||theta_new - theta_old|| / ||theta_old||` | Effective learning rate per epoch |
| `ema_divergence` | `||theta_EMA - theta_online|| / ||theta_online||` | How far EMA shadows lag behind online model |
| `learning_rate` | Current LR from scheduler | Actual learning rate at this epoch |
| `epoch_time_sec` | Wall-clock seconds for the epoch | Training throughput (useful for comparing runs) |

**`ema_divergence` dynamics**: EMA update is `theta_EMA <- 0.999 * theta_EMA + 0.001 * theta_online`. If the model changes rapidly, EMA lags and divergence grows. Plateau means convergence. The EMA model is what's used for inference, so divergence matters for sample quality.

### Validation

| Field | What it measures |
|-------|-----------------|
| `val_loss` | Validation set adaptive-weighted loss |
| `val_raw_loss` | Validation set raw loss (pre-adaptive) |

Validation runs every `val_every_n_epochs` (default: 10). Available only at those epochs. Compare `val_raw_loss` vs `raw_loss_mean` to detect overfitting. In a dataset of ~1,117 training samples, overfitting is a real concern.

### Sampling Stats

Stored in `sampling` dict. These are **sanity checks** — they should be constant across epochs:

| Key | Expected value | What it measures |
|-----|----------------|-----------------|
| `t_mean` | ~0.53 | Mean of logit-normal(mu=-0.4, sigma=1.0) |
| `t_std` | ~0.18 | Std of the time distribution |
| `h_mean` | ~0.06 | Mean of h = t - r |
| `h_zero_frac` | 0.75 | Fraction of FM samples (should equal data_proportion) |

If any of these drift, there's a bug in the sampling code.

### Per-Channel Loss (periodic)

`per_channel_loss`: array of 4 floats, present only every `diag_every_n_epochs` epochs. The per-channel contribution to `||V - v_c||^p` across the 4 VAE latent channels. Large disparities suggest the model struggles with certain structural modes.

### Generated Samples (merged from sample archives)

Present at collection epochs (default: every 25). Stored under `"generated_samples"` key:

```json
{
  "generated_samples": {
    "stats": {
      "nfe_1": {"mean": [c0, c1, c2, c3], "std": [...], "min": float, "max": float},
      "nfe_5": {...},
      "nfe_10": {...}
    },
    "nfe_consistency": {
      "mse_1vs2": float,
      "mse_1vs5": float,
      "mse_1vs10": float,
      "cosine_1vs2": float,
      "cosine_1vs5": float,
      "cosine_1vs10": float
    }
  }
}
```

**Key metrics:**

| Field | What it measures | Convergence indicator |
|-------|-----------------|----------------------|
| `stats.nfe_1.mean` | Per-channel mean of 1-NFE samples | Should converge toward training data stats (~0 if normalised) |
| `stats.nfe_1.std` | Per-channel std of 1-NFE samples | Should converge toward ~1.0 if normalised |
| `nfe_consistency.mse_1vs10` | MSE between 1-NFE and 10-step Euler | **Decreasing** means 1-NFE is converging to multi-step quality |
| `nfe_consistency.cosine_1vs10` | Cosine similarity between 1-NFE and 10-step | **Approaching 1.0** means 1-NFE direction matches multi-step |

**Interpretation**: The MeanFlow distillation goal is to make 1-NFE as good as multi-step Euler. If `mse_1vs10` decreases over training while `cosine_1vs10` approaches 1.0, the distillation is working. If the gap widens, the model is overfitting the 1-step objective at the expense of consistency.

Raw `.pt` archives are at `samples/generated_samples/epoch_NNNN.pt` with tensors for each NFE step plus the fixed noise, enabling post-hoc VAE decoding via `experiments/cli/decode_samples.py`.

---

## Analysis Principles

### 1. Explore First, Conclude Later

Load the data, compute summary statistics for every metric, identify which ones change and which are constant. Let the data speak before forming opinions. Start broad, then zoom into anomalies.

### 2. Reason About Cross-Metric Correlations

The metrics are deeply interlinked. Informative correlations to investigate:

- **`raw_loss_mean` vs `cosine_sim_V_vc`**: If raw loss decreases AND cosine similarity increases, learning is working on both magnitude and direction. If raw loss decreases but cosine similarity is flat, the model is only improving magnitudes (bad — direction matters more for generation).

- **`raw_loss_fm_mean` vs `raw_loss_mf_mean`**: FM should always be lower. If both decrease in parallel, both objectives are being learned. If FM decreases but MF doesn't, the MeanFlow correction is failing.

- **`adaptive_weight.mean` vs `raw_loss_mean`**: These should track each other (adaptive weight IS the raw loss, with eps offset). If they diverge, there's a numerical issue.

- **`velocity_norms.u_norm / velocity_norms.target_v_norm`**: The norm ratio should approach 1.0. Plot this ratio over epochs.

- **`ema_divergence` vs `relative_update_norm`**: ema_divergence is the integral; relative_update_norm is the derivative. If the derivative drops to near-zero but EMA divergence hasn't plateaued, the EMA is still catching up to a recently-stabilised model (normal with decay=0.999).

- **`grad_clip_fraction` vs `out` block norm**: Both reflect gradient magnitude. If both decrease together, the model's predictions are getting closer to the target — learning is working.

- **`r_embed` gradient norm trajectory**: If growing, the model is becoming more sensitive to the r conditioning, meaning the MF term (r != t) is becoming more important. Expected in mid-to-late training.

- **`x_hat_stats.std` vs `target_v_norm`**: If x-hat stats become extreme while target norms are stable, the model is generating out-of-distribution predictions.

### 3. Think About Training Phases

Diffusion/flow models typically exhibit distinct learning phases:

- **Early** (epochs 0-50): High clip_frac, large `out` norms, cosine_sim near 0, raw loss may initially increase as the model escapes the zero-init minimum.
- **Mid** (epochs 50-200): clip_frac drops, gradient redistribution to deeper blocks, cosine_sim rises above 0.5, raw loss steadily decreasing.
- **Late** (epochs 200+): Low clip_frac, stable norms, ema_divergence plateau, raw loss decreasing slowly. Model is fine-tuning details.

Identify which phase the run is in and whether transitions are occurring at reasonable epochs.

### 4. Quantify, Don't Just Describe

When you identify a trend, compute its magnitude. Examples of good quantification:
- "raw_loss_mean decreased by 42% (23.5 -> 13.6) over 500 epochs"
- "cosine_sim_V_vc improved from 0.02 to 0.71 (epoch 0 -> 499)"
- "u_norm/target_v_norm ratio converged from 0.3 to 0.95"
- "Pearson correlation between grad_clip_fraction and out block norm: r=0.94"

### 5. Flag Known Failure Modes

Known issues from previous runs to check for:

- **Adaptive weight masking divergence**: `loss_mean` appears constant at ~1.0 while `raw_loss_mean` (or `adaptive_weight.mean`) grows by orders of magnitude. The model is getting worse but the adaptive normalisation hides it.
- **Warmup starvation**: If `learning_rate` is near 0 for a large fraction of training, effective learning is delayed.
- **MF term domination**: If `raw_loss_mf_mean >> raw_loss_fm_mean` by many orders of magnitude and growing, the MeanFlow JVP correction is unstable.
- **Mode collapse**: `x_hat_stats.std` drops to near 0 — the model predicts the same output for all inputs. Check sample images.
- **Dead gradients on deep blocks**: If `middle_block` and `down_block_3` norms are zero or negligible for the entire run, the bottleneck is not contributing to learning. This can happen with zero-init residual connections.

### 6. Compare Timescales

Some metrics change fast (grad_clip_fraction can shift in 10 epochs), others slowly (ema_divergence integrates over entire history). When two fast metrics correlate, that suggests a direct causal link. When a fast metric correlates with a slow one, think about lagged dynamics.

### 7. Use Sample Images

Check the generated sample images in `samples/epoch_NNNN/`:
- `latent_channels.png` — 4-channel latent visualization. Do the channels show spatial structure? Do they resemble brain anatomy?
- `samples_grid.png` — VAE-decoded MRI slices. Do they look like brains? Is there structure or just noise/blobs?

Compare samples across epochs to see visual improvement. If metrics suggest convergence but samples look bad, there may be a decode/denormalization issue.

---

## Plotting Guidance

When generating figures, produce a single multi-panel figure with shared x-axis (epoch) for easy cross-metric visual comparison. Save as PNG at 150+ DPI in `/tmp/` and show the path.

Recommended panels (adapt based on what the data shows):

1. **Raw loss** (`raw_loss_mean`) — use log scale if dynamic range is large
2. **Cosine similarity** (`cosine_sim_V_vc`) — with y-axis [0, 1]
3. **Velocity norm ratio** (`u_norm / target_v_norm`) — should converge to 1.0
4. **FM vs MF loss** (`raw_loss_fm_mean` and `raw_loss_mf_mean` overlaid, log scale)
5. **Gradient clipping fraction** (`grad_clip_fraction`)
6. **Selected block gradient norms** (log scale) — at minimum: `out`, `conv_in`, `middle_block`, `r_embed`
7. **EMA divergence** and **relative update norm** (dual y-axis or separate panels)
8. **Relative error** (`relative_error`)

If a metric appears constant (e.g., `h_zero_frac = 0.75`), do not waste a panel on it. Mention its constancy in text and move on.

For older runs that lack the new fields (velocity_norms, cosine similarities, etc.), work with what's available and note which diagnostics are missing.

---

## Training Config Context

The training config uses:
- AdamW, lr=1e-4, beta1=0.9, **beta2=0.95**, weight_decay=0.0
- gradient_clip_norm=1.0
- batch_size=16 per GPU, 2 GPUs (DDP), accumulate_grad_batches=4 -> **effective batch=128**
- data_proportion=0.75 (75% FM, 25% MF samples)
- EMA decay=0.999
- Mixed precision bf16
- Logit-normal time sampling: mu=-0.4, sigma=1.0
- Lp loss with p=2.0, adaptive weighting, **norm_eps=1.0**
- x-prediction type, t_min=0.05
- lr_schedule=constant (no warmup)
- UNet: 178M params, channels=[64,128,256,512], attention at levels 2,3
- JVP strategy: finite_difference (h=1e-3, required with gradient checkpointing)
- max_epochs=500 -> ~4,500 optimizer steps total
- ~1,117 training samples, ~124 validation samples

---

## MeanFlow Mathematical Background (Quick Reference)

**Interpolation**: `z_t = (1-t)*z_0 + t*eps`

**Target**: `v_c = eps - z_0` (conditional velocity)

**x-prediction**: model outputs x_hat, converted to `u = (z_t - x_hat) / t`

**JVP tangent**: `v_tilde = u(z_t, t, t)` (model's own predicted instantaneous velocity, detached)

**Compound velocity**: `V = u + (t-r) * sg[du/dt]`, where `du/dt` is the JVP of u w.r.t. (t, z_t) in direction (1, v_tilde)

**Loss**: `||V - v_c||^p` with optional adaptive weighting `w = 1/(sg[||V - v_c||^p] + eps)`

**1-NFE generation**: `z_0 = model(noise, r=0, t=1)` — the model directly outputs the predicted clean latent.

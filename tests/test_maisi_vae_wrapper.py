"""Phase 0 gate tests for the MAISI VAE wrapper.

7 CRITICAL tests (P0-T1 through P0-T7) — Phase 0 passes when all are green.
5 INFORMATIONAL tests (P0-T8 through P0-T12) — negative controls and diagnostics.
"""

from __future__ import annotations

import json
import logging
from pathlib import Path

import pytest
import torch
from omegaconf import OmegaConf

from neuromf.wrappers.maisi_vae import MAISIVAEConfig, MAISIVAEWrapper

logger = logging.getLogger(__name__)


# ---------------------------------------------------------------------------
# Module-scoped fixtures (load VAE once, share across all tests)
# ---------------------------------------------------------------------------


@pytest.fixture(scope="module")
def merged_config(base_config):
    """Merge base.yaml with vae_validation.yaml."""
    vae_cfg_path = Path(__file__).parent.parent / "configs" / "vae_validation.yaml"
    vae_cfg = OmegaConf.load(vae_cfg_path)
    merged = OmegaConf.merge(base_config, vae_cfg)
    OmegaConf.resolve(merged)
    return merged


@pytest.fixture(scope="module")
def vae_config(merged_config) -> MAISIVAEConfig:
    """Build MAISIVAEConfig from merged config."""
    return MAISIVAEConfig.from_omegaconf(merged_config)


@pytest.fixture(scope="module")
def vae_wrapper(vae_config, device) -> MAISIVAEWrapper:
    """Load the MAISI VAE wrapper (module-scoped to avoid reloading)."""
    if not vae_config.weights_path or not Path(vae_config.weights_path).exists():
        pytest.skip("VAE weights not available")
    return MAISIVAEWrapper(vae_config, device=device)


@pytest.fixture(scope="module")
def validation_metrics(merged_config) -> dict:
    """Load pre-computed metrics.json or run inline validation.

    Reads from the metrics file generated by ``validate_vae.py``. If
    unavailable, runs an inline validation on 20 IXI volumes.
    """
    metrics_dir = Path(merged_config.output.metrics_dir)
    metrics_path = metrics_dir / "metrics.json"

    if metrics_path.exists():
        logger.info("Loading pre-computed metrics from %s", metrics_path)
        return json.loads(metrics_path.read_text())

    # Inline fallback: run validation on the spot
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    if device.type != "cuda":
        pytest.skip("No GPU available for inline VAE validation")

    from neuromf.data.mri_preprocessing import (
        build_mri_preprocessing_from_config,
        get_ixi_file_list,
    )

    ixi_root = merged_config.paths.ixi_t1
    if not Path(ixi_root).exists():
        pytest.skip("IXI dataset not available")

    import numpy as np

    from neuromf.metrics.ssim_psnr import compute_psnr, compute_ssim_3d

    vae_config = MAISIVAEConfig.from_omegaconf(merged_config)
    vae = MAISIVAEWrapper(vae_config, device=device)
    transform = build_mri_preprocessing_from_config(merged_config)
    file_list = get_ixi_file_list(ixi_root, n_volumes=merged_config.data.n_validation)

    per_volume = []
    for data_dict in file_list:
        processed = transform(data_dict)
        x = processed["image"].unsqueeze(0).to(device)
        x_hat = vae.reconstruct(x)
        psnr_val = compute_psnr(x, x_hat)
        ssim_val = compute_ssim_3d(x.cpu(), x_hat.cpu())
        per_volume.append({"ssim": ssim_val, "psnr": psnr_val})
        del x, x_hat
        torch.cuda.empty_cache()

    mean_ssim = float(np.mean([m["ssim"] for m in per_volume]))
    mean_psnr = float(np.mean([m["psnr"] for m in per_volume]))

    # Cache for future runs
    metrics_dir.mkdir(parents=True, exist_ok=True)
    result = {"mean_ssim": mean_ssim, "mean_psnr": mean_psnr, "per_volume": per_volume}
    metrics_path.write_text(json.dumps(result, indent=2))

    return result


# ---------------------------------------------------------------------------
# Tests (P0-T1 through P0-T7)
# ---------------------------------------------------------------------------


@pytest.mark.phase0
@pytest.mark.critical
def test_P0_T1_vae_weights_load(vae_wrapper: MAISIVAEWrapper) -> None:
    """P0-T1: MAISI VAE weights load without error."""
    assert vae_wrapper is not None
    assert vae_wrapper.model is not None
    n_params = sum(p.numel() for p in vae_wrapper.model.parameters())
    assert n_params > 0, "VAE has no parameters"
    logger.info("VAE loaded with %d parameters", n_params)


@pytest.mark.phase0
@pytest.mark.critical
def test_P0_T2_encode_shape(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T2: Encode produces correct latent shape (B,4,32,32,32)."""
    x = torch.randn(1, 1, 128, 128, 128, device=device)
    z = vae_wrapper.encode(x)
    assert z.shape == (1, 4, 32, 32, 32), f"Expected (1,4,32,32,32), got {z.shape}"


@pytest.mark.phase0
@pytest.mark.critical
def test_P0_T3_decode_shape(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T3: Decode produces correct output shape (B,1,128,128,128)."""
    z = torch.randn(1, 4, 32, 32, 32, device=device)
    x_hat = vae_wrapper.decode(z)
    assert x_hat.shape == (1, 1, 128, 128, 128), f"Expected (1,1,128,128,128), got {x_hat.shape}"


@pytest.mark.phase0
@pytest.mark.critical
def test_P0_T4_ssim_above_threshold(validation_metrics: dict) -> None:
    """P0-T4: Mean SSIM over validation volumes > 0.90."""
    mean_ssim = validation_metrics["mean_ssim"]
    logger.info("Mean SSIM: %.4f", mean_ssim)
    assert mean_ssim > 0.90, f"Mean SSIM {mean_ssim:.4f} <= 0.90 threshold"


@pytest.mark.phase0
@pytest.mark.critical
def test_P0_T5_psnr_above_threshold(validation_metrics: dict) -> None:
    """P0-T5: Mean PSNR over validation volumes > 30.0 dB."""
    mean_psnr = validation_metrics["mean_psnr"]
    logger.info("Mean PSNR: %.2f dB", mean_psnr)
    assert mean_psnr > 30.0, f"Mean PSNR {mean_psnr:.2f} <= 30.0 dB threshold"


@pytest.mark.phase0
@pytest.mark.critical
def test_P0_T6_vae_frozen(vae_wrapper: MAISIVAEWrapper) -> None:
    """P0-T6: All VAE parameters have requires_grad=False."""
    for name, param in vae_wrapper.model.named_parameters():
        assert not param.requires_grad, f"Parameter {name} has requires_grad=True"


@pytest.mark.phase0
@pytest.mark.critical
def test_P0_T7_bf16_inference(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T7: bf16 autocast produces finite outputs (no NaN/Inf)."""
    if device.type != "cuda":
        pytest.skip("bf16 autocast requires CUDA")

    x = torch.randn(1, 1, 128, 128, 128, device=device)

    with torch.autocast("cuda", dtype=torch.bfloat16):
        z = vae_wrapper.encode(x)
        x_hat = vae_wrapper.decode(z)

    assert torch.isfinite(z).all(), "Latent contains NaN or Inf under bf16"
    assert torch.isfinite(x_hat).all(), "Reconstruction contains NaN or Inf under bf16"


# ---------------------------------------------------------------------------
# INFORMATIONAL tests (P0-T8 through P0-T12)
# ---------------------------------------------------------------------------


@pytest.mark.phase0
@pytest.mark.informational
def test_P0_T8_noise_input_low_ssim(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T8: Gaussian noise input produces SSIM < 0.5 (VAE is not identity)."""
    from neuromf.metrics.ssim_psnr import compute_ssim_3d

    x_noise = torch.randn(1, 1, 128, 128, 128, device=device).clamp(0.0, 1.0)
    x_hat = vae_wrapper.reconstruct(x_noise)
    ssim = compute_ssim_3d(x_noise.cpu(), x_hat.cpu())
    logger.info("Noise input SSIM: %.4f (expect < 0.5)", ssim)
    assert ssim < 0.5, f"Noise SSIM {ssim:.4f} >= 0.5 — VAE may be acting as identity"


@pytest.mark.phase0
@pytest.mark.informational
def test_P0_T9_encoding_stochasticity(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T9: Seeded encode is deterministic; unseeded encode is stochastic.

    Documents that ``encode_stage_2_inputs`` calls ``sampling(z_mu, z_sigma)``
    with ``torch.randn_like``, making encoding stochastic. The underlying
    ``model.encode()`` returns ``(z_mu, z_sigma)`` for deterministic access.
    """
    x = torch.randn(1, 1, 128, 128, 128, device=device).clamp(0.0, 1.0)

    # Seeded: deterministic
    torch.manual_seed(42)
    z1 = vae_wrapper.encode(x)
    torch.manual_seed(42)
    z2 = vae_wrapper.encode(x)
    assert torch.allclose(z1, z2, atol=1e-6), "Seeded encodes should match"

    # Document posterior sigma via model.encode()
    with torch.no_grad():
        use_autocast = vae_wrapper.config.norm_float16 and x.is_cuda
        with torch.amp.autocast(device_type=x.device.type, enabled=use_autocast):
            z_mu, z_sigma = vae_wrapper.model.encode(x)
        z_sigma = z_sigma.float()
    mean_sigma = z_sigma.mean().item()
    logger.info("Mean posterior sigma: %.6f (non-trivial => stochastic)", mean_sigma)
    assert mean_sigma > 1e-6, (
        f"Posterior sigma too small ({mean_sigma}) — encoding may not be stochastic"
    )

    # Unseeded: stochastic (different samples)
    z3 = vae_wrapper.encode(x)
    z4 = vae_wrapper.encode(x)
    assert not torch.allclose(z3, z4, atol=1e-6), "Unseeded encodes should differ"


@pytest.mark.phase0
@pytest.mark.informational
def test_P0_T10_scale_factor_matters(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T10: Correct scale factor yields higher SSIM than sf=1.0."""
    from neuromf.metrics.ssim_psnr import compute_ssim_3d

    # Try real IXI volume first; fall back to clamped random
    try:
        from neuromf.data.mri_preprocessing import (
            build_mri_preprocessing_from_config,
            get_ixi_file_list,
        )

        vae_cfg_path = Path(__file__).parent.parent / "configs" / "vae_validation.yaml"
        base_cfg_path = Path(__file__).parent.parent / "configs" / "base.yaml"
        from omegaconf import OmegaConf as _OC

        _cfg = _OC.merge(_OC.load(base_cfg_path), _OC.load(vae_cfg_path))
        _OC.resolve(_cfg)
        ixi_files = get_ixi_file_list(_cfg.paths.ixi_t1, n_volumes=1)
        transform = build_mri_preprocessing_from_config(_cfg)
        processed = transform(ixi_files[0])
        x = processed["image"].unsqueeze(0).to(device)
        logger.info("Using real IXI volume for scale factor test")
    except Exception:
        x = torch.randn(1, 1, 128, 128, 128, device=device).clamp(0.0, 1.0)
        logger.info("Using random input for scale factor test (IXI unavailable)")

    z = vae_wrapper.encode(x)

    # Correct scale factor via wrapper
    x_hat_correct = vae_wrapper.decode(z)
    ssim_correct = compute_ssim_3d(x.cpu(), x_hat_correct.cpu())

    # Wrong scale factor (1.0): bypass wrapper
    z_wrong = z / 1.0  # no scaling
    use_autocast = vae_wrapper.config.norm_float16 and z.is_cuda
    with torch.no_grad(), torch.amp.autocast(device_type=z.device.type, enabled=use_autocast):
        x_hat_wrong = vae_wrapper.model.decode_stage_2_outputs(z_wrong)
    x_hat_wrong = x_hat_wrong.float()
    ssim_wrong = compute_ssim_3d(x.cpu(), x_hat_wrong.cpu())

    logger.info("SSIM correct sf=%.4f, wrong sf=%.4f", ssim_correct, ssim_wrong)
    assert ssim_correct > ssim_wrong, (
        f"Correct sf SSIM ({ssim_correct:.4f}) should exceed wrong sf SSIM ({ssim_wrong:.4f})"
    )


@pytest.mark.phase0
@pytest.mark.informational
def test_P0_T11_latent_statistics(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T11: Per-channel latent std in [0.01, 100] and mean is finite."""
    x = torch.randn(1, 1, 128, 128, 128, device=device).clamp(0.0, 1.0)
    z = vae_wrapper.encode(x)  # (1, 4, 32, 32, 32)

    for ch in range(z.shape[1]):
        ch_data = z[0, ch]
        ch_mean = ch_data.mean().item()
        ch_std = ch_data.std().item()
        logger.info("Channel %d: mean=%.4f, std=%.4f", ch, ch_mean, ch_std)
        assert torch.isfinite(ch_data).all(), f"Channel {ch} contains non-finite values"
        assert 0.01 <= ch_std <= 100.0, f"Channel {ch} std={ch_std} outside [0.01, 100]"


@pytest.mark.phase0
@pytest.mark.informational
def test_P0_T12_ood_blank_input(vae_wrapper: MAISIVAEWrapper, device: torch.device) -> None:
    """P0-T12: Zeros and constant 0.5 produce finite latents and reconstructions."""
    for val, label in [(0.0, "zeros"), (0.5, "constant_0.5")]:
        x = torch.full((1, 1, 128, 128, 128), val, device=device)
        z = vae_wrapper.encode(x)
        x_hat = vae_wrapper.decode(z)
        assert torch.isfinite(z).all(), f"{label}: latent contains non-finite values"
        assert torch.isfinite(x_hat).all(), f"{label}: reconstruction contains non-finite values"
        logger.info(
            "%s: latent range [%.4f, %.4f], recon range [%.4f, %.4f]",
            label,
            z.min().item(),
            z.max().item(),
            x_hat.min().item(),
            x_hat.max().item(),
        )

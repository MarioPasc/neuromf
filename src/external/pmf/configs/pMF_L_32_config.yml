dataset:
    root: DATA_ROOT

model:
    model_str: MiT_L_32
    cfg_beta: 1.0

    P_mean: 0.0
    P_std: 0.8
    noise_scale: 4.0

    lpips: True
    lpips_lambda: 0.4
    convnext: True
    convnext_lambda: 0.1
    perceptual_max_t: 0.6

training:
    log_per_step: 100
    checkpoint_per_epoch: 20

    num_epochs: 320
    batch_size: 1024
    learning_rate: 0.001 # for muon
    warmup_epochs: 0
    
    adam_b2: 0.95
    # multiple ema
    ema_type: 'edm'
    ema_val: [500, 1000, 2000]

    sample_per_epoch: 5
    fid_per_epoch: 10

    seed: 42

fid:
    device_batch_size: 40
    num_samples: 50000
    
    cache_ref: FID_CACHE_REF

sampling:
    # sample parameters evaluated during training
    num_steps: 1
    omega: 7.5
    t_min: 0.2
    t_max: 0.6

dataset:
    image_size: 512

logging:
    use_wandb: False # set to True to use wandb
    wandb_project: 'YOUR PROJECT' # change to your wandb project
    wandb_entity: 'YOUR ENTITY' # paste your wandb entity here
    wandb_tags: # use your own tags
        - YOUR TAGS
    wandb_notes: 'YOUR NOTES' # your wandb notes

load_from: ''

# Phase 4 training config (skeleton prepared in Phase 3)
# Full training configuration for MeanFlow on brain MRI latents

# Trainer infrastructure
# For multi-GPU DDP, override devices/strategy in picasso/train_meanflow.yaml
trainer:
  devices: 1          # Number of GPUs (1 = single-GPU, N = DDP with N GPUs)
  strategy: auto      # "auto" for single-GPU, "ddp" for multi-GPU
  accelerator: auto   # "auto" detects GPU/CPU, "gpu" to force CUDA

# Model
unet:
  spatial_dims: 3
  in_channels: 4
  out_channels: 4
  channels: [64, 128, 256, 512]
  attention_levels: [false, false, true, true]
  num_res_blocks: 2
  num_head_channels: [0, 0, 32, 32]
  norm_num_groups: 32
  resblock_updown: true
  transformer_num_layers: 1
  use_flash_attention: false
  with_conditioning: false
  prediction_type: x
  t_min: 0.05
  gradient_checkpointing: true

# Training
training:
  batch_size: 24
  lr: 1.0e-4
  weight_decay: 1.0e-4
  optimizer: adamw
  betas: [0.9, 0.95]            # reference: adam_beta2=0.95
  max_epochs: 500
  warmup_steps: 0               # reference: no warmup
  lr_schedule: cosine             # "constant" | "cosine" | "linear"
  gradient_clip_norm: 1.0
  gradient_checkpointing: true
  mixed_precision: bf16
  log_every_n_steps: 50
  val_every_n_epochs: 10
  save_every_n_epochs: 10
  num_workers: 0
  prefetch_factor: null
  split_ratio: 0.9
  split_seed: 42
  accumulate_grad_batches: 1      # gradient accumulation steps
  divergence_threshold: 100.0      # 0=disabled; halts if raw_loss > Nx min raw loss
  divergence_grace_steps: 100     # skip checking first N optimizer steps
  augmentation:                   # Phase B (disabled by default)
    enabled: false
    flip_prob: 0.5
    flip_axes: [0, 1, 2]
    rotate90_prob: 0.5
    rotate90_axes: [[1, 2]]       # axial-plane rotation only
    gaussian_noise_prob: 0.0
    gaussian_noise_std_fraction: 0.05
    intensity_scale_prob: 0.0
    intensity_scale_factors: 0.05

# MeanFlow
meanflow:
  p: 2.0
  adaptive: true
  norm_eps: 0.01                  # match reference class default
  lambda_mf: 1.0
  prediction_type: x
  t_min: 0.05
  jvp_strategy: exact
  fd_step_size: 1.0e-3
  channel_weights: null
  norm_p: 0.5
  spatial_mask_ratio: 0.0         # Phase C (0.0=off; e.g. 0.5=mask 50%)

# Time sampling
time_sampling:
  distribution: logit_normal
  mu: -0.4
  sigma: 1.0
  t_min: 0.001
  data_proportion: 0.75          # reference: 75% pure FM samples

# EMA
ema:
  decay: 0.9999

# Sampling / logging (legacy keys, kept for backward compat)
sample_every_n_epochs: 25
n_samples_per_log: 8
latent_spatial_size: 48

# Sample collection (replaces inline sample generation)
sample_collector:
  enabled: true
  collect_every_n_epochs: 25    # matches sample_every_n_epochs
  n_samples: 8                  # matches n_samples_per_log
  nfe_steps: [1, 2, 5, 10]     # multi-NFE for comparison
  seed: 42                      # fixed noise for evolution tracking

# VAE for post-training decode (used by decode_samples.py CLI)
vae:
  spatial_dims: 3
  in_channels: 1
  out_channels: 1
  latent_channels: 4
  num_channels: [64, 128, 256]
  num_res_blocks: [2, 2, 2]
  norm_num_groups: 32
  norm_eps: 1.0e-6
  attention_levels: [false, false, false]
  with_encoder_nonlocal_attn: false
  with_decoder_nonlocal_attn: false
  use_checkpointing: false
  use_convtranspose: false
  norm_float16: false
  num_splits: 6
  dim_split: 1
  scale_factor: 0.96240234375
  downsample_factor: 4

# Diagnostics
diagnostics:
  enabled: false
  every_n_epochs: 25

# Paths (will be merged with base.yaml)
paths:
  latents_dir: ${paths.results_root}/latents
  checkpoints_dir: ${paths.results_root}/training_checkpoints
  logs_dir: ${paths.results_root}/phase_4/logs
  samples_dir: ${paths.results_root}/phase_4/samples
  diagnostics_dir: ${paths.results_root}/phase_4/diagnostics
